{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "Action: 1\n",
      "Observation: 4\n",
      "Reward: 0.0\n",
      "False False\n",
      "Action: 3\n",
      "Observation: 4\n",
      "Reward: 0.0\n",
      "False False\n",
      "Action: 3\n",
      "Observation: 4\n",
      "Reward: 0.0\n",
      "False False\n",
      "Action: 0\n",
      "Observation: 0\n",
      "Reward: 0.0\n",
      "False False\n",
      "Action: 0\n",
      "Observation: 4\n",
      "Reward: 0.0\n",
      "False False\n",
      "Action: 0\n",
      "Observation: 8\n",
      "Reward: 0.0\n",
      "False False\n",
      "Action: 2\n",
      "Observation: 4\n",
      "Reward: 0.0\n",
      "False False\n",
      "Action: 3\n",
      "Observation: 5\n",
      "Reward: 0.0\n",
      "False True\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE gym different environments\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "#import time\n",
    "#print(gym.__version__)\n",
    "\n",
    "#env = gym.make('LunarLander')\n",
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make('FrozenLake-v1')\n",
    "observation = env.reset(seed=42)\n",
    "action_size = env.action_space.n\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "done = False\n",
    "n_episodes = 1\n",
    "for _ in range(n_episodes):\n",
    "    while not done:\n",
    "        action = np.random.randint(0, action_size)\n",
    "        observation, reward, done, info,_ = env.step(action)\n",
    "        print('Action:', action)\n",
    "        print('Observation:', observation)\n",
    "        print('Reward:', reward)\n",
    "        print(info,done)\n",
    "        #env.render()\n",
    "    done = False\n",
    "    env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make q_learning train function for 1 episode\n",
    "def q_learning_episode(env, q_table):\n",
    "    epsilon = 0.1\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    done = False\n",
    "    steps = 0\n",
    "    state = env.reset(seed=42)\n",
    "    while not done:\n",
    "        # POLICY\n",
    "        if type(state)==int:    # state troca entre 0 e (0,prob=1), não sei porque, dai todos os if, else\n",
    "            previous_state=state\n",
    "        else:\n",
    "            previous_state=state[0]\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(0,4)\n",
    "        else:\n",
    "            if type(state)==int:\n",
    "                action =np.argmax(q_table[state])\n",
    "            else:\n",
    "                action =np.argmax(q_table[state[0]])\n",
    "        state, reward, done, _ ,_= env.step(action)\n",
    "        reward-= 1\n",
    "        if state in [5, 7, 11, 12]:\n",
    "            reward -= 20\n",
    "        elif state == 15:\n",
    "            reward += 50\n",
    "        next_state=state\n",
    "        new_state_max = np.max(q_table[next_state])\n",
    "        #q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])\n",
    "        q_table[previous_state,action] += alpha*(reward + gamma*new_state_max - q_table[previous_state,action])\n",
    "        # in case the episode is taking too many steps to finish\n",
    "        steps+=1\n",
    "        if steps >= 400:\n",
    "            break        \n",
    "    return (q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "def fazer_q_table():\n",
    "    env = gym.make('FrozenLake-v1',is_slippery=True)\n",
    "\n",
    "    number_of_actions = env.action_space.n\n",
    "    number_of_states = env.observation_space.n\n",
    "\n",
    "# Initialize Q-Table\n",
    "    q_table = np.array(np.zeros((number_of_states, number_of_actions)))\n",
    "    print('Q Table - SHAPE:', q_table.shape)\n",
    "    # number of episodes to train the agent\n",
    "    n_episodes = 120\n",
    "    for e in range(n_episodes):\n",
    "        q_table = q_learning_episode(env, q_table)\n",
    "    print(q_table)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Table - SHAPE: (16, 4)\n",
      "[[-2.50846591  4.27894192 -3.71142622 -2.37125588]\n",
      " [-9.66971259 -5.68731152 -4.30678344 -1.59522252]\n",
      " [-3.24968072 -3.45197884 -3.4730495  -3.42559341]\n",
      " [-5.58197547 -6.15861563 -4.17550059 -3.8821978 ]\n",
      " [10.87339061 -3.47494663 -3.87421463 -4.07641912]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-5.66297833 -7.47295498 -7.77478929 -7.17848514]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-5.7639      3.88669167 -2.84253713 18.5894936 ]\n",
      " [ 3.88366504 27.55252195 -2.19        2.37272171]\n",
      " [32.30712088  2.3676687  -3.28898747  4.2846791 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 2.67252731 -0.19       24.81691368 -5.52662602]\n",
      " [ 6.78400104 45.9204229   9.08448195  1.97356105]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "Action: 1\n",
      "Observation: 4\n",
      "Reward: 0.0\n",
      "Action: 0\n",
      "Observation: 8\n",
      "Reward: 0.0\n",
      "Action: 3\n",
      "Observation: 8\n",
      "Reward: 0.0\n",
      "Action: 3\n",
      "Observation: 9\n",
      "Reward: 0.0\n",
      "Action: 1\n",
      "Observation: 10\n",
      "Reward: 0.0\n",
      "Action: 0\n",
      "Observation: 14\n",
      "Reward: 0.0\n",
      "Action: 1\n",
      "Observation: 15\n",
      "Reward: 1.0\n",
      "Número de passos: 7\n"
     ]
    }
   ],
   "source": [
    "# Show results of training\n",
    "\n",
    "def evaluate_q_learning(env, q_table):\n",
    "    def policy(q_table, observation):\n",
    "        action = np.argmax(q_table[observation])# replace this with the right policy\n",
    "        return action\n",
    "    observation = env.reset(seed=42)\n",
    "    observation=0\n",
    "    done = False\n",
    "    nstep=0\n",
    "    while not done:\n",
    "        action = policy(q_table, observation)\n",
    "        observation, reward, done,_,_ = env.step(action)\n",
    "        print('Action:', action)\n",
    "        print('Observation:', observation)\n",
    "        print('Reward:', reward)\n",
    "        env.render()\n",
    "        nstep+=1      # Contar número de passos\n",
    "    print(f'Número de passos: {nstep}')\n",
    "    env.close()\n",
    "env = gym.make('FrozenLake-v1',render_mode=\"human\",is_slippery=True)\n",
    "observation = env.reset(seed=42)\n",
    "q_table=fazer_q_table()             # refazer q table\n",
    "# use q_table to evaluate function\n",
    "evaluate_q_learning(env,q_table)\n",
    "\n",
    "with open('sample.txt', 'w') as f:    # Guardar q table\n",
    "    f.write(str(q_table))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sistemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0072dcc2df830b042d5d7447fe5080206d55ec0c42aabe900855a12c66e0b05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
